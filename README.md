# Directional Silent Speech Recognition
Group members: Aditi Ravindra, Helen Liu, Rachel Jee

# What is this project about?
The directional silent speech recognition project aims to create an ear-worn system for identifying unvoiced human commands that is an alternative to voice-based interactions. The specific voice commands that the project focuses on classifying correctly are directional based, using the four primary directions of “left”, “right”, “up”, and “down”, for data collection and testing. The core idea behind this is that with an IMU sensor placed on the lower jaw, the acceleration and gyroscopic signals are used to break down the articulation of a word and further used to determine the directional command the user is saying via jaw motion data without relying on any verbal or audio noise. The importance of this project comes from the drawbacks of voice-based interactions, which can be unreliable in noisy environments, disruptive to one’s surroundings, and potentially compromise one’s privacy. Silent speech recognition overcomes these issues while also maintaining the usability of voice-based interaction and communication, and additionally allowing people with speech disorders or others who cannot produce any sound but can still articulate words with their jaw to also take advantage of voice-based-like operations.

See Project Report.pdf for more in-depth information on process and evaluation.


